{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e512a49d",
   "metadata": {},
   "source": [
    "# Image classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab88c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports to be used through the notebook\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d39101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523ef60",
   "metadata": {},
   "source": [
    "## Data (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f230f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a transformation to apply to the items of the dataset (that are, by default, in PIL image format). The standardization will help with training.\n",
    "\n",
    "MEAN_MNIST = (0.1307,)\n",
    "STD_MNIST = (0.3081,)\n",
    "\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN_MNIST, STD_MNIST)])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='C:\\\\Users\\\\PabloHueso\\\\OneDrive - CompoSistemas, S.L\\\\Documents\\\\Master\\\\M2\\\\ENAC\\\\Adversarial-attacks\\\\Datasets', train=True,\n",
    "                                        download=True, transform=transform_mnist)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='C:\\\\Users\\\\PabloHueso\\\\OneDrive - CompoSistemas, S.L\\\\Documents\\\\Master\\\\M2\\\\ENAC\\\\Adversarial-attacks\\\\Datasets', train=False,\n",
    "                                       download=True, transform=transform_mnist)\n",
    "\n",
    "\n",
    "#trainset.__len__() # yields 60000\n",
    "#trainset.__getitem__(0) # tuple (image, class); image is a tensor 1x28x28 and class is an int\n",
    "#trainset.__getitem__(0)[0].shape #yields torch.Size([1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28556f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size train loader:  1875  | test loader:  313\n"
     ]
    }
   ],
   "source": [
    "# Trainloaders are dataset wrappers used to access them in a batched way. Of course this is very useful for NN training.\n",
    "batch_size = 32\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Size train loader: \", len(trainloader), \" | test loader: \", len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67044aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trainloader)) # batches are lists of two tensors, one containing features and other labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4011dce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:      3             6             6             0 \n",
      "\n",
      "Image shape (number of channels, height, width): (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img * STD_MNIST[0] + MEAN_MNIST[0]  # Proper unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')  # MNIST is grayscale + original tensor shape is (1,28,28) so  np.transpose(img, (1, 2, 0)) gets it to shape (28,28,1) \n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images[:4], labels[:4]\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print('Labels:     ','             '.join(f'{labels[j]}' for j in range(4)), '\\n')\n",
    "print(f'Image shape (number of channels, height, width): {tuple(images[0].shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e0a6b",
   "metadata": {},
   "source": [
    "## First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559ea274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, 10, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(1690, output_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial shape is [B, 1, 28, 28]\n",
    "        x = self.conv(x)\n",
    "        # After convoluting, shape is [B, 10, 26, 26]\n",
    "        x = nn.ReLU()(x)\n",
    "        # After ReLU, shape is [B, 10, 26, 26] (it's an element-wise operation)\n",
    "        x = self.pool(x)\n",
    "        # After pooling, shape is [B, 10, 13, 13]\n",
    "        x = nn.Flatten()(x)\n",
    "        # After flattening shape is [B, 1690] (naturally, 1690 = 10x13x13)\n",
    "        x = self.fc(x)\n",
    "        # Final shape is [B, 10] (we have logits for 10 possible classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f5a05b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca1eb01",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9051ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels=1\n",
    "output_shape=10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def build_and_train_model(archi=SimpleCNN, trainloader=trainloader, input_shape=input_channels, output_shape=output_shape, optimizer=optim.SGD, criterion=criterion, device=\"cpu\", n_epochs=10, lr=0.01):\n",
    " \n",
    "  # When building a model, you assign it to a particular device by setting: model = model.to(device); device is indicated by a string, like \"cpu\" our \"cuda:0\"\n",
    "  model = archi(input_shape,output_shape).to(device)\n",
    "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "  for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, (x,y) in enumerate(trainloader):\n",
    "      # Same as for the model, you have to assign a device for your inputs and your label, for example: x = x.to(device)\n",
    "      inputs, labels = x.to(device), y.to(device)\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch} / {n_epochs} | Loss: {running_loss / len(trainloader)}')\n",
    "    running_loss = 0.0\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d859cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 10 | Loss: 0.17810321882081528\n",
      "Epoch 1 / 10 | Loss: 0.07494120440216114\n",
      "Epoch 2 / 10 | Loss: 0.05965796685898676\n",
      "Epoch 3 / 10 | Loss: 0.05069920963292631\n",
      "Epoch 4 / 10 | Loss: 0.043284731868988215\n",
      "Epoch 5 / 10 | Loss: 0.039605296323479464\n",
      "Epoch 6 / 10 | Loss: 0.03373726984369568\n",
      "Epoch 7 / 10 | Loss: 0.030677522122949205\n",
      "Epoch 8 / 10 | Loss: 0.027577684416323123\n",
      "Epoch 9 / 10 | Loss: 0.02553154419551526\n",
      "Training with cuda:0 lasts: 1.71 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = build_and_train_model(device=device)\n",
    "print(f\"Training with {device} lasts: {np.round((time.time()-start_time)/60,2)} minutes\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73055ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once your model has been train, you have to evaluate it on the test set.\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "      for X, y in dataloader:\n",
    "          pred = model(X.to(device))\n",
    "          loss = loss_fn(pred, y.to(device)).item()\n",
    "          test_loss += loss\n",
    "          ### TODO: compute the model final prediction and the number of correct predictions.\n",
    "          # Your current output is the for each class to be the right one.\n",
    "          prediction = pred.argmax(axis=1)\n",
    "          # To compute the accuracy score, we need to know how many correct predictions we got.\n",
    "          correct += (prediction == y.to(device)).sum().item()\n",
    "          ###\n",
    "    test_loss /= len(dataloader)\n",
    "    correct /= len(dataloader.dataset)\n",
    "    print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13a6ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: Accuracy: 97.7%, Avg loss: 0.080042 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = test_loop(testloader, model, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c20446",
   "metadata": {},
   "source": [
    "## Example of attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(iter(trainloader)) #x_batch is of dimension [B, C, H, W] = [512, 1, 28, 28]. Elements of it (like x_batch[0]) are of dimension [C, H, W] = [1, 28, 28].\n",
    "x_single = x_batch[0].unsqueeze(0)  # Add batch dimension back (shape: [1, C, H, W])\n",
    "\n",
    "\n",
    "x_single = x_single.to(device)\n",
    "output = model(x_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c641f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = x_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6c7d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_batch(model, x_batch): \n",
    "    # Compute jacobian at a point (w.r.t. x only)\n",
    "    B = len(x_batch) # batch size\n",
    "    J_all = torch.func.jacrev(model)(x_batch) #yields torch.Size([B, 10, B, 1, 28, 28])\n",
    "    J_diag = J_all.diagonal(dim1=0, dim2=2)   # (B,10,1,28,28)\n",
    "    J_diag = J_diag.reshape(B, 10, -1)        # (B,10,784)\n",
    "    return J_diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff831ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 32, 1, 28, 28])\n",
      "torch.Size([10, 1, 28, 28, 32])\n",
      "torch.Size([32, 10, 784])\n"
     ]
    }
   ],
   "source": [
    " # Compute jacobian at a point (w.r.t. x only)\n",
    "B = len(x_batch) # batch size\n",
    "J_all = torch.func.jacrev(model)(x_batch) #yields torch.Size([B, 10, B, 1, 28, 28])\n",
    "print(J_all.shape)\n",
    "J_diag = J_all.diagonal(dim1=0, dim2=2)   # (10,1,28,28,B)\n",
    "print(J_diag.shape)\n",
    "J_diag = J_diag.reshape(B, 10, -1)        # (B,10,784)\n",
    "print(J_diag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "925ca869",
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian_b = jacobian_batch(model, x_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c514106",
   "metadata": {},
   "outputs": [],
   "source": [
    "J_pinv = torch.linalg.pinv(jacobian_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05f1b120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_pinv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37c7558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e0d1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = torch.topk(input=model_output, k=3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35c98c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipping_vector(model, x_batch, attacked_class): \n",
    "    model_output = model(x_batch) # [B, c]\n",
    "    B, c = model_output.shape\n",
    "\n",
    "    assert 2 <= attacked_class <= c, \"attacked_class must be in 2,...,c\"\n",
    "\n",
    "    largest_logits = torch.topk(model_output, k=attacked_class, dim=1).indices\n",
    "    flipping_vector = torch.zeros(model_output.shape) # zero tensor of shape [B, c]\n",
    "\n",
    "    # i: index of max logit per sample (argmax) -> [B]\n",
    "    i = largest_logits[:, 0]\n",
    "\n",
    "    # j: index of attacked_class-th largest logit -> [B]\n",
    "    j = largest_logits[:, -1] \n",
    "    \n",
    "\n",
    "    flipping = torch.zeros_like(model_output)  # [B, c]\n",
    "\n",
    "    batch_indices = torch.arange(B) #vector [0, 1, ... , B-1]\n",
    "\n",
    "    flipping[batch_indices, i] = -1\n",
    "    flipping[batch_indices, j] = 1\n",
    "\n",
    "    return flipping # [B, c] -> [B, c, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "027180c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flips = flipping_vector(model, x_single, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7ede2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_pinv.shape\n",
    "flips.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be99d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = torch.bmm(J_pinv, flips.unsqueeze(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EntornoDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
